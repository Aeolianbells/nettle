C nettle, low-level cryptographics library
C
C Copyright (C) 2013 Niels MÃ¶ller
C
C The nettle library is free software; you can redistribute it and/or modify
C it under the terms of the GNU Lesser General Public License as published by
C the Free Software Foundation; either version 2.1 of the License, or (at your
C option) any later version.
C
C The nettle library is distributed in the hope that it will be useful, but
C WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
C or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
C License for more details.
C
C You should have received a copy of the GNU Lesser General Public License
C along with the nettle library; see the file COPYING.LIB.  If not, write to
C the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
C MA 02111-1301, USA.

	.file "sha512-compress.asm"
	.fpu	neon

define(<STATE>, <r0>)
define(<INPUT>, <r1>)
define(<K>, <r2>)
define(<SA>, <d16>)
define(<SB>, <d17>)
define(<SC>, <d18>)
define(<SD>, <d19>)
define(<SE>, <d20>)
define(<SF>, <d21>)
define(<SG>, <d22>)
define(<SH>, <d23>)
define(<W>, <d24>)
define(<T0>, <d25>)

define(<COUNT>, <r3>)

C Used for data load
define(<I0>, <r4>)
define(<I1>, <r5>)
define(<I2>, <r6>)
define(<I3>, <r7>)
define(<I4>, <r8>)
define(<DST>, <r10>)
define(<SHIFT>, <r12>)
define(<IT>, <r14>)

C FIXME: More opportunities for parallelism, at least do s0 and s1 xors,
C or expand two words at a time.
define(<EXPN>, <
	vldr		W, [sp, #+eval(8*$1)]
	vldr		T0, [sp, #+eval(8*(($1 + 14) % 16))]
	vshl.i64	d0, T0, #45
	vshr.u64	d2, T0, #19
	vshl.i64	d1, T0, #3
	vshr.u64	d3, T0, #61
	vadd.i64	q0, q0, q1
	vshr.u64	T0, T0, #6
	veor		T0, T0, d0
	veor		T0, T0, d1
	vadd.i64	W, W, T0
	vldr		T0, [sp, #+eval(8*(($1 + 9) % 16))]
	vadd.i64	W, W, T0
	vldr		T0, [sp, #+eval(8*(($1 + 1) % 16))]
	vshl.i64	d0, T0, #63
	vshr.u64	d2, T0, #1
	vshl.i64	d1, T0, #56
	vshr.u64	d3, T0, #8
	vadd.i64	q0, q0, q1
	vshr.u64	T0, T0, #7
	veor		T0, T0, d0
	veor		T0, T0, d1
	vadd.i64	W, W, T0
	vstr		W, [sp, #+eval(8*$1)]
>)

C ROUND(A,B,C,D,E,F,G,H,i)
C
C H += S1(E) + Choice(E,F,G) + K + W
C D += H
C H += S0(A) + Majority(A,B,C)
C
C Where
C
C S1(E) = E<<<50 ^ E<<<46 ^ E<<<23
C S0(A) = A<<<36 ^ A<<<30 ^ A<<<25
C Choice (E, F, G) = G^(E&(F^G))
C Majority (A,B,C) = (A&B) + (C&(A^B))

C FIXME: More opportunities for parallelism, at least do S0 and S1 xors.
define(<ROUND>, <
	vshl.i64	d0, $5, #50
	vshr.u64	d2, $5, #14
	vshl.i64	d1, $5, #46
	vshr.u64	d3, $5, #18
	vadd.i64	q0, q0, q1
	vshl.i64	d2, $5, #23
	vshr.u64	d3, $5, #41
	vadd.i64	d2, d2, d3
	veor		d0, d0, d1
	veor		d0, d0, d2
	vadd.i64	$8, $8, d0
	veor		d0, $6, $7
	vand		d0, d0, $5
	veor		d0, d0, $7
	vadd.i64	$8,$8, d0
	vldr		d0, [K,#eval(8*$9)]
	vadd.i64	$8, $8, W
	vadd.i64	$8, $8, d0
	vadd.i64	$4, $4, $8

	vshl.i64	d0, $1, #36
	vshr.u64	d2, $1, #28
	vshl.i64	d1, $1, #30
	vshr.u64	d3, $1, #34
	vadd.i64	q0, q0, q1
	vshl.i64	d2, $1, #25
	vshr.u64	d3, $1, #39
	vadd.i64	d2, d2, d3
	veor		d0, d0, d1
	veor		d0, d0, d2
	vadd.i64	$8, $8, d0
	vand		d0, $1, $2
	veor		d1, $1, $2
	vadd.i64	$8, $8, d0
	vand		d1, d1, $3
	vadd.i64	$8, $8, d1
>)

define(<NOEXPN>, <
	vldr	W, [INPUT, #eval(8*$1)]
>)

	C void
	C _nettle_sha512_compress(uint64_t *state, const uint8_t *input, const uint64_t *k)

	.text
	.align 2

PROLOGUE(_nettle_sha512_compress)
	push	{r4,r5,r6,r7,r8,r10,r14}
	sub	sp, sp, #128
	
	ands	SHIFT, INPUT, #7
	and	INPUT, INPUT, #-8
	vld1.8	{d0}, [INPUT :64]
	addne	INPUT, INPUT, #8
	addeq	SHIFT, SHIFT, #8
	lsl	SHIFT, SHIFT, #3

	C Put right shift in d2 and d3, aka q1
	neg	SHIFT, SHIFT
	vmov.i32	d2, #0
	vmov.32		d2[0], SHIFT
	vmov		d3, d2
	C Put left shift in d4 and d5, aka q2
	add		SHIFT, SHIFT, #64
	vmov.i32	d4, #0
	vmov.32		d4[0], SHIFT
	vmov		d5, d4
	vshl.u64	d0, d0, d2

	mov	DST, sp
	mov	COUNT, #4
.Lcopy:
	C Set w[i] <-- w[i-1] >> RSHIFT + w[i] << LSHIFT
	vld1.8		{d16,d17,d18,d19}, [INPUT :64]!
	vshl.u64	q3, q8, q1	C Right shift
	vshl.u64	q8, q8, q2	C Left shift
	veor		d16, d16, d0
	veor		d17, d17, d6
	vrev64.8	q8, q8
	vshl.u64	q0, q9, q1	C Right shift
	vshl.u64	q9, q9, q2	C Left shift
	veor		d18, d18, d7
	veor		d19, d19, d0
	vrev64.8	q9, q9
	subs	COUNT, COUNT, #1
	vst1.64		{d16,d17,d18,d19}, [DST]!
	vmov		d0, d1
	bne	.Lcopy

	mov	COUNT,#2
	mov	INPUT, sp

	vldm	STATE, {SA,SB,SC,SD,SE,SF,SG,SH}

.Loop1:
	NOEXPN(0) ROUND(SA,SB,SC,SD,SE,SF,SG,SH, 0)
	NOEXPN(1) ROUND(SH,SA,SB,SC,SD,SE,SF,SG, 1)
	NOEXPN(2) ROUND(SG,SH,SA,SB,SC,SD,SE,SF, 2)
	NOEXPN(3) ROUND(SF,SG,SH,SA,SB,SC,SD,SE, 3)
	NOEXPN(4) ROUND(SE,SF,SG,SH,SA,SB,SC,SD, 4)
	NOEXPN(5) ROUND(SD,SE,SF,SG,SH,SA,SB,SC, 5)
	NOEXPN(6) ROUND(SC,SD,SE,SF,SG,SH,SA,SB, 6)
	NOEXPN(7) ROUND(SB,SC,SD,SE,SF,SG,SH,SA, 7)
	subs	COUNT,#1
	add	INPUT, INPUT, #64
	add	K, K, #64
	bne	.Loop1

	mov	COUNT, #4
.Loop2:

	EXPN( 0) ROUND(SA,SB,SC,SD,SE,SF,SG,SH,  0)
	EXPN( 1) ROUND(SH,SA,SB,SC,SD,SE,SF,SG,  1)
	EXPN( 2) ROUND(SG,SH,SA,SB,SC,SD,SE,SF,  2)
	EXPN( 3) ROUND(SF,SG,SH,SA,SB,SC,SD,SE,  3)
	EXPN( 4) ROUND(SE,SF,SG,SH,SA,SB,SC,SD,  4)
	EXPN( 5) ROUND(SD,SE,SF,SG,SH,SA,SB,SC,  5)
	EXPN( 6) ROUND(SC,SD,SE,SF,SG,SH,SA,SB,  6)
	EXPN( 7) ROUND(SB,SC,SD,SE,SF,SG,SH,SA,  7)
	EXPN( 8) ROUND(SA,SB,SC,SD,SE,SF,SG,SH,  8)
	EXPN( 9) ROUND(SH,SA,SB,SC,SD,SE,SF,SG,  9)
	EXPN(10) ROUND(SG,SH,SA,SB,SC,SD,SE,SF, 10)
	EXPN(11) ROUND(SF,SG,SH,SA,SB,SC,SD,SE, 11)
	EXPN(12) ROUND(SE,SF,SG,SH,SA,SB,SC,SD, 12)
	EXPN(13) ROUND(SD,SE,SF,SG,SH,SA,SB,SC, 13)
	EXPN(14) ROUND(SC,SD,SE,SF,SG,SH,SA,SB, 14)
	subs	COUNT, COUNT, #1
	EXPN(15) ROUND(SB,SC,SD,SE,SF,SG,SH,SA, 15)
	add	K, K, #128
	bne	.Loop2

	vld1.64		{d24,d25,d26,d27}, [STATE]
	vadd.i64	SA, SA, d24
	vadd.i64	SB, SB, d25
	vadd.i64	SC, SC, d26
	vadd.i64	SD, SD, d27
	vst1.64		{SA,SB,SC,SD}, [STATE]!
	vld1.64		{d24,d25,d26,d27}, [STATE]
	vadd.i64	SE, SE, d24
	vadd.i64	SF, SF, d25
	vadd.i64	SG, SG, d26
	vadd.i64	SH, SH, d27
	vst1.64		{SE,SF,SG,SH}, [STATE]!

	add		sp, sp, #128
	pop		{r4,r5,r6,r7,r8,r10,pc}
EPILOGUE(_nettle_sha512_compress)

divert(-1)
define shastate
p/x $d16.u64
p/x $d17.u64
p/x $d18.u64
p/x $d19.u64
p/x $d20.u64
p/x $d21.u64
p/x $d22.u64
p/x $d23.u64
end
